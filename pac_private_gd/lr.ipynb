{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94df8ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "bank_marketing = pd.read_csv(os.path.join('.', 'bank', 'bank-full.csv'), sep=\";\")\n",
    "# data (as pandas dataframes)\n",
    "# DataFrame of features (categorical + numeric)\n",
    "X = bank_marketing.drop(columns=[\"y\"])\n",
    "y = bank_marketing[\"y\"]                  # Series target: \"yes\"/\"no\"\n",
    "y = (y.astype(str).str.lower() == \"yes\").astype(int).to_numpy()\n",
    "\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "num_cols = X.columns.difference(cat_cols)\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(with_mean=True, with_std=False), list(num_cols)),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"onehot\", OneHotEncoder(\n",
    "                handle_unknown=\"ignore\", sparse_output=False)),\n",
    "            (\"scaler\", StandardScaler(with_mean=True, with_std=False)),\n",
    "        ]),\n",
    "            list(cat_cols)),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "def pca(X_train, X_test, whiten, tol=1e-6):\n",
    "    pca = PCA(whiten=False, random_state=42)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    # get the non-zero eigenvalue components\n",
    "    non_zero_var_indices = pca.explained_variance_ > tol\n",
    "    X_train_pca = X_train_pca[:, non_zero_var_indices]\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    X_test_pca = X_test_pca[:, non_zero_var_indices]\n",
    "    if whiten:\n",
    "        X_train_whitened = X_train_pca / np.sqrt(pca.explained_variance_[non_zero_var_indices])\n",
    "        X_test_whitened = X_test_pca / np.sqrt(pca.explained_variance_[non_zero_var_indices])\n",
    "        return X_train_whitened, X_test_whitened\n",
    "    else:\n",
    "        return X_train_pca, X_test_pca\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train = ct.fit_transform(X_train)\n",
    "X_test = ct.transform(X_test)\n",
    "X_train, X_test = pca(X_train, X_test, whiten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "122b06ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import gammaln\n",
    "\n",
    "def exact_var_1d(c):\n",
    "    \"\"\"\n",
    "    Exact variance of\n",
    "        Y = (sum_i m_i * c_i) / (sum_i m_i),\n",
    "    where m_i ~ Bernoulli(0.5) i.i.d., and Y=0 when sum_i m_i = 0.\n",
    "\n",
    "    Uses log-sum-exp vectorization for numerical stability and speed.\n",
    "    \"\"\"\n",
    "    c = np.asarray(c, dtype=float)\n",
    "    n = c.size\n",
    "    a = np.sum(c)\n",
    "    b = np.sum(c**2)\n",
    "\n",
    "    # ----- compute E_invK = E[1/K * 1_{K≥1}] -----\n",
    "    k = np.arange(1, n + 1)\n",
    "    # log(C(n,k)) = log(n!) - log(k!) - log((n-k)!)\n",
    "    log_comb = gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)\n",
    "    log_terms = log_comb - np.log(k) - n * np.log(2)\n",
    "\n",
    "    # log-sum-exp for numerical stability\n",
    "    max_log = np.max(log_terms)\n",
    "    E_invK = np.exp(max_log) * np.sum(np.exp(log_terms - max_log))\n",
    "\n",
    "    # probability K ≥ 1\n",
    "    p_nonzero = 1 - 2 ** (-n)\n",
    "\n",
    "    # ----- compute variance -----\n",
    "    term1 = (b * n - a**2) / (n**2 * (n - 1))\n",
    "    var_conditional = term1 * (n * E_invK - p_nonzero)\n",
    "    var_mean = (a / n)**2 * p_nonzero * (1 - p_nonzero)\n",
    "\n",
    "    return var_conditional + var_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6877ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "from scipy.special import lambertw\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def optimal_eta(mu, T, C, e0, var):\n",
    "    if e0 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    if var == 0:\n",
    "        # minimize (1-eta * mu)^T * e0, this is achieved at eta = 1/mu\n",
    "        return 1/mu\n",
    "\n",
    "    alpha = mu * T\n",
    "    # beta = (1+C) * var /  mu\n",
    "    # a = alpha * e0**2 / beta + 1\n",
    "    a = mu * alpha * T * e0**2 / ((1 + C) * var)\n",
    "    \n",
    "    if a>500:\n",
    "        # eta = 1/alpha * (np.log(a) - np.log(a)/a) # very good approximation when a is large\n",
    "        eta = 1/alpha * (np.log1p(a) - np.log1p(a)/(a+1)) # very good approximation when a is large\n",
    "    else:\n",
    "        eta = 1/alpha * (a+1 - lambertw(np.exp(a+1)).real)\n",
    "        \n",
    "    return eta\n",
    "\n",
    "def pac_private_logistic_regression(X, y, mu, mi_budget, T=10, privacy_aware=True):\n",
    "\n",
    "    def objective(w):\n",
    "        N = X.shape[0]\n",
    "        z = X @ w\n",
    "        y_hat = sigmoid(z)\n",
    "        loss = -(1/N) * np.sum(y * np.log(y_hat) + (1 - y) * np.log1p(- y_hat))\n",
    "        reg_term = (mu / 2) * np.sum(w ** 2)\n",
    "        return loss + reg_term\n",
    "    \n",
    "    w0 = np.zeros(X.shape[1])\n",
    "    e0 = minimize(objective, w0, method='L-BFGS-B', tol=1e-10).x\n",
    "\n",
    "    N, d = X.shape\n",
    "    w = np.zeros(d)\n",
    "    losses = []\n",
    "    C = d * T / 2.0 / mi_budget\n",
    "\n",
    "    for _ in range(T):\n",
    "        z = X @ w\n",
    "        y_hat = sigmoid(z)\n",
    "\n",
    "        # --- Per-sample gradients ---\n",
    "        per_sample_grads = (y_hat - y)[:, np.newaxis] * X  # shape (N, d)\n",
    "\n",
    "        for d_i in range(d):\n",
    "            \n",
    "            # grad_i_var = .exact_var_1d(per_sample_grads[:, d_i])\n",
    "            grad_i_var = np.var(np.array([per_sample_grads[np.random.rand(N) < 0.5, d_i].mean() for _ in range(128)])) # scalar\n",
    "            grad = per_sample_grads[np.random.rand(N) < 0.5, d_i].mean() + mu * w[d_i] + np.sqrt(C * grad_i_var) * np.random.randn()\n",
    "\n",
    "            lr = optimal_eta(mu=mu, T=T, C=C if privacy_aware else 0, e0=e0[d_i], var=grad_i_var)\n",
    "\n",
    "            w[d_i] = w[d_i] - lr * grad\n",
    "        \n",
    "        z = X @ w\n",
    "        y_hat = sigmoid(z)\n",
    "        loss = -(1/N) * np.sum(y * np.log(y_hat) + (1 - y) * np.log1p(- y_hat))\n",
    "\n",
    "        reg_term = (mu / 2) * np.sum(w ** 2)\n",
    "        losses.append(loss + reg_term)\n",
    "        print(f'Train loss: {losses[-1]:.4f}')\n",
    "\n",
    "    return w, losses\n",
    "\n",
    "def evaluate_model(X, y, w):\n",
    "    z = X @ w\n",
    "    y_hat = sigmoid(z)\n",
    "    y_pred = (y_hat > 0.5).astype(int)\n",
    "    accuracy = np.mean(y_pred == y)\n",
    "    return accuracy\n",
    "\n",
    "def model_loss(X, y, w, mu):\n",
    "    N = X.shape[0]\n",
    "    z = X @ w\n",
    "    y_hat = sigmoid(z)\n",
    "    loss = -(1/N) * np.sum(y * np.log(y_hat) + (1 - y) * np.log1p(- y_hat))\n",
    "    reg_term = (mu / 2) * np.sum(w ** 2)\n",
    "    return loss + reg_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d85566d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7089\n",
      "Train loss: 0.7161\n",
      "Train loss: 0.7269\n",
      "Train loss: 0.7186\n",
      "Train loss: 0.7081\n",
      "Train loss: 0.6907\n",
      "Train loss: 0.6914\n",
      "Train loss: 0.6904\n",
      "Train loss: 0.6920\n",
      "Train loss: 0.6945\n",
      "Test accuracy: 0.6855\n",
      "Train loss: 0.6858\n",
      "Train loss: 0.6868\n",
      "Train loss: 0.6894\n",
      "Train loss: 0.6895\n",
      "Train loss: 0.6923\n",
      "Train loss: 0.6955\n",
      "Train loss: 0.6927\n",
      "Train loss: 0.6911\n",
      "Train loss: 0.6925\n",
      "Train loss: 0.6964\n",
      "Test accuracy: 0.6893\n",
      "Train loss: 0.6988\n",
      "Train loss: 0.6926\n",
      "Train loss: 0.6890\n",
      "Train loss: 0.6881\n",
      "Train loss: 0.6964\n",
      "Train loss: 0.6944\n",
      "Train loss: 0.6904\n",
      "Train loss: 0.6911\n",
      "Train loss: 0.6904\n",
      "Train loss: 0.6900\n",
      "Test accuracy: 0.6838\n",
      "Train loss: 0.6899\n",
      "Train loss: 0.6898\n",
      "Train loss: 0.6860\n",
      "Train loss: 0.6888\n",
      "Train loss: 0.7040\n",
      "Train loss: 0.6949\n",
      "Train loss: 0.6907\n",
      "Train loss: 0.6941\n",
      "Train loss: 0.7080\n",
      "Train loss: 0.7122\n",
      "Test accuracy: 0.6664\n",
      "Train loss: 0.6885\n",
      "Train loss: 0.6871\n",
      "Train loss: 0.6891\n",
      "Train loss: 0.6943\n",
      "Train loss: 0.7042\n",
      "Train loss: 0.6945\n",
      "Train loss: 0.6924\n",
      "Train loss: 0.6985\n",
      "Train loss: 0.6945\n",
      "Train loss: 0.6912\n",
      "Test accuracy: 0.6343\n",
      "Train loss: 0.6922\n",
      "Train loss: 0.6924\n",
      "Train loss: 0.6889\n",
      "Train loss: 0.6882\n",
      "Train loss: 0.6875\n",
      "Train loss: 0.6887\n",
      "Train loss: 0.6892\n",
      "Train loss: 0.6890\n",
      "Train loss: 0.7046\n",
      "Train loss: 0.6925\n",
      "Test accuracy: 0.5249\n",
      "Train loss: 0.6926\n",
      "Train loss: 0.6915\n",
      "Train loss: 0.6977\n",
      "Train loss: 0.6957\n",
      "Train loss: 0.6870\n",
      "Train loss: 0.7006\n",
      "Train loss: 0.6889\n",
      "Train loss: 0.7070\n",
      "Train loss: 0.7098\n",
      "Train loss: 0.7388\n",
      "Test accuracy: 0.3249\n",
      "Train loss: 0.6867\n",
      "Train loss: 0.6881\n",
      "Train loss: 0.6949\n",
      "Train loss: 0.6948\n",
      "Train loss: 0.7010\n",
      "Train loss: 0.6972\n",
      "Train loss: 0.6955\n",
      "Train loss: 0.6958\n",
      "Train loss: 0.6889\n",
      "Train loss: 0.6884\n",
      "Test accuracy: 0.6846\n",
      "Train loss: 0.6857\n",
      "Train loss: 0.6896\n",
      "Train loss: 0.6904\n",
      "Train loss: 0.6986\n",
      "Train loss: 0.6979\n",
      "Train loss: 0.7239\n",
      "Train loss: 0.7177\n",
      "Train loss: 0.7163\n",
      "Train loss: 0.7228\n",
      "Train loss: 0.7471\n",
      "Test accuracy: 0.7087\n",
      "Train loss: 0.6956\n",
      "Train loss: 0.7190\n",
      "Train loss: 0.7107\n",
      "Train loss: 0.7037\n",
      "Train loss: 0.7004\n",
      "Train loss: 0.6996\n",
      "Train loss: 0.6938\n",
      "Train loss: 0.6991\n",
      "Train loss: 0.6948\n",
      "Train loss: 0.6954\n",
      "Test accuracy: 0.5297\n",
      "Average test accuracy over 10 runs: 0.6132 ± 0.1146\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "for _ in range(10):\n",
    "    w, losses = pac_private_logistic_regression(X_train, y_train, mu=1.0, mi_budget=1/256, T=10)\n",
    "    test_accuracy = evaluate_model(X_test, y_test, w)\n",
    "    accs.append(test_accuracy)\n",
    "\n",
    "    print(f'Test accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "print(f'Average test accuracy over 10 runs: {np.mean(accs):.4f} ± {np.std(accs):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pac_lr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
