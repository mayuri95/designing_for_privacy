{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3d26857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def logistic_regression_dataset_synthesis(N, d):\n",
    "    w = np.random.randn(d)\n",
    "    X = np.random.randn(N, d)\n",
    "    logits = X @ w\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    y = (np.random.rand(N) < probs).astype(np.float32)\n",
    "    return X, y, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e23717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y, w= logistic_regression_dataset_synthesis(1000, 10)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "970d211d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8850, Test Accuracy: 0.9200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=1000, random_state=42, fit_intercept=False)\n",
    "model.fit(X_train, y_train)\n",
    "train_acc = model.score(X_train, y_train)\n",
    "test_acc = model.score(X_test, y_test)\n",
    "print(f\"Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94df8ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# bank_marketing = pd.read_csv(os.path.join('.', 'bank', 'bank-full.csv'), sep=\";\")\n",
    "# # data (as pandas dataframes)\n",
    "# # DataFrame of features (categorical + numeric)\n",
    "# X = bank_marketing.drop(columns=[\"y\"])\n",
    "# y = bank_marketing[\"y\"]                  # Series target: \"yes\"/\"no\"\n",
    "# y = (y.astype(str).str.lower() == \"yes\").astype(int).to_numpy()\n",
    "\n",
    "# cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "# num_cols = X.columns.difference(cat_cols)\n",
    "\n",
    "# X = X.drop(columns=cat_cols)\n",
    "\n",
    "# ct = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         (\"num\", StandardScaler(with_mean=True, with_std=True), list(num_cols)),\n",
    "#         # (\"cat\", Pipeline([\n",
    "#         #     (\"onehot\", OneHotEncoder(\n",
    "#         #         handle_unknown=\"ignore\", sparse_output=False)),\n",
    "#         #     (\"scaler\", StandardScaler(with_mean=True, with_std=False)),\n",
    "#         # ]),\n",
    "#         #     list(cat_cols)),\n",
    "#     ],\n",
    "#     remainder=\"drop\",\n",
    "# )\n",
    "\n",
    "# def pca_wrapper(X_train, X_test, whiten, tol=1e-6):\n",
    "#     pca = PCA(whiten=False, random_state=42)\n",
    "#     X_train_pca = pca.fit_transform(X_train)\n",
    "#     # get the non-zero eigenvalue components\n",
    "#     non_zero_var_indices = pca.explained_variance_ > tol\n",
    "#     X_train_pca = X_train_pca[:, non_zero_var_indices]\n",
    "#     X_test_pca = pca.transform(X_test)\n",
    "#     X_test_pca = X_test_pca[:, non_zero_var_indices]\n",
    "#     if whiten:\n",
    "#         X_train_whitened = X_train_pca / np.sqrt(pca.explained_variance_[non_zero_var_indices])\n",
    "#         X_test_whitened = X_test_pca / np.sqrt(pca.explained_variance_[non_zero_var_indices])\n",
    "#         return X_train_whitened, X_test_whitened\n",
    "#     else:\n",
    "#         return X_train_pca, X_test_pca\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# X_train = ct.fit_transform(X_train)\n",
    "# X_test = ct.transform(X_test)\n",
    "# # X_train, X_test = pca_wrapper(X_train, X_test, whiten=True)\n",
    "# # pca = PCA(whiten=False, random_state=42, n_components=0.99)\n",
    "# # X_train = pca.fit_transform(X_train)\n",
    "# # X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "122b06ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import gammaln\n",
    "\n",
    "def exact_var_1d(c):\n",
    "    \"\"\"\n",
    "    Exact variance of\n",
    "        Y = (sum_i m_i * c_i) / (sum_i m_i),\n",
    "    where m_i ~ Bernoulli(0.5) i.i.d., and Y=0 when sum_i m_i = 0.\n",
    "\n",
    "    Uses log-sum-exp vectorization for numerical stability and speed.\n",
    "    \"\"\"\n",
    "    c = np.asarray(c, dtype=float)\n",
    "    n = c.size\n",
    "    a = np.sum(c)\n",
    "    b = np.sum(c**2)\n",
    "\n",
    "    # ----- compute E_invK = E[1/K * 1_{K≥1}] -----\n",
    "    k = np.arange(1, n + 1)\n",
    "    # log(C(n,k)) = log(n!) - log(k!) - log((n-k)!)\n",
    "    log_comb = gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)\n",
    "    log_terms = log_comb - np.log(k) - n * np.log(2)\n",
    "\n",
    "    # log-sum-exp for numerical stability\n",
    "    max_log = np.max(log_terms)\n",
    "    E_invK = np.exp(max_log) * np.sum(np.exp(log_terms - max_log))\n",
    "\n",
    "    # probability K ≥ 1\n",
    "    p_nonzero = 1 - 2 ** (-n)\n",
    "\n",
    "    # ----- compute variance -----\n",
    "    term1 = (b * n - a**2) / (n**2 * (n - 1))\n",
    "    var_conditional = term1 * (n * E_invK - p_nonzero)\n",
    "    var_mean = (a / n)**2 * p_nonzero * (1 - p_nonzero)\n",
    "\n",
    "    return var_conditional + var_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6877ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "from scipy.special import lambertw\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def optimal_eta(mu, T, C, e0, var):\n",
    "    if e0 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    if var == 0:\n",
    "        # minimize (1-eta * mu)^T * e0, this is achieved at eta = 1/mu\n",
    "        return 1/mu\n",
    "\n",
    "    alpha = mu * T\n",
    "    beta = (1+C) * var /  mu\n",
    "    a = alpha * e0**2 / beta\n",
    "    \n",
    "    if a>500:\n",
    "        print(\"Using approximation for large a\")\n",
    "        # eta = 1/alpha * (np.log(a) - np.log(a)/a) # very good approximation when a is large\n",
    "        eta = 1/alpha * (np.log1p(a) - np.log1p(a)/(a+1)) # very good approximation when a is large\n",
    "    else:\n",
    "        eta = 1/alpha * (a+1 - lambertw(np.exp(a+1)).real)\n",
    "        \n",
    "    return eta\n",
    "\n",
    "def pac_private_logistic_regression(X, y, mu, mi_budget, T=10, privacy_aware=True):\n",
    "\n",
    "    def objective(w):\n",
    "        N = X.shape[0]\n",
    "        z = X @ w\n",
    "        y_hat = sigmoid(z)\n",
    "        loss = -(1/N) * np.sum(y * np.log(y_hat) + (1 - y) * np.log1p(- y_hat))\n",
    "        reg_term = (mu / 2) * np.sum(w ** 2)\n",
    "        return loss + reg_term\n",
    "    \n",
    "    e0 = minimize(objective, np.zeros(X.shape[1]), method='L-BFGS-B', tol=1e-10).x\n",
    "\n",
    "    N, d = X.shape\n",
    "    w = np.zeros(d)\n",
    "    losses = []\n",
    "    # C = d * T / 2.0 / mi_budget\n",
    "    C = 0\n",
    "\n",
    "    for _ in range(T):\n",
    "        z = X @ w\n",
    "        y_hat = sigmoid(z)\n",
    "\n",
    "        # --- Per-sample gradients ---\n",
    "        per_sample_grads = (y_hat - y)[:, np.newaxis] * X  # shape (N, d)\n",
    "\n",
    "        for d_i in range(d):\n",
    "            \n",
    "            grad_i_var = exact_var_1d(per_sample_grads[:, d_i])\n",
    "            # grad_i_var = np.var(np.array([per_sample_grads[np.random.rand(N) < 0.5, d_i].mean() for _ in range(128)])) # scalar\n",
    "            grad = per_sample_grads[np.random.rand(N) < 0.5, d_i].mean() + mu * w[d_i] + np.sqrt(C * grad_i_var) * np.random.randn()\n",
    "\n",
    "            lr = optimal_eta(mu=mu, T=T, C=C if privacy_aware else 0, e0=e0[d_i], var=grad_i_var)\n",
    "            # lr = 0.1\n",
    "\n",
    "            w[d_i] = w[d_i] - lr * grad\n",
    "        \n",
    "        z = X @ w\n",
    "        y_hat = sigmoid(z)\n",
    "        loss = -(1/N) * np.sum(y * np.log(y_hat) + (1 - y) * np.log1p(- y_hat))\n",
    "\n",
    "        reg_term = (mu / 2) * np.sum(w ** 2)\n",
    "        losses.append(loss + reg_term)\n",
    "        print(f'Train loss: {losses[-1]:.4f}')\n",
    "\n",
    "    return w, losses\n",
    "\n",
    "def evaluate_model(X, y, w):\n",
    "    z = X @ w\n",
    "    y_hat = sigmoid(z)\n",
    "    y_pred = (y_hat > 0.5).astype(int)\n",
    "    accuracy = np.mean(y_pred == y)\n",
    "    return accuracy\n",
    "\n",
    "def model_loss(X, y, w, mu):\n",
    "    N = X.shape[0]\n",
    "    z = X @ w\n",
    "    y_hat = sigmoid(z)\n",
    "    loss = -(1/N) * np.sum(y * np.log(y_hat) + (1 - y) * np.log1p(- y_hat))\n",
    "    reg_term = (mu / 2) * np.sum(w ** 2)\n",
    "    return loss + reg_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d85566d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3975\n",
      "Train loss: 0.3634\n",
      "Train loss: 0.3290\n",
      "Train loss: 0.2957\n",
      "Train loss: 0.2881\n",
      "Train loss: 0.2801\n",
      "Train loss: 0.2841\n",
      "Train loss: 0.2731\n",
      "Train loss: 0.2879\n",
      "Train loss: 0.2814\n",
      "Train loss: 0.2908\n",
      "Train loss: 0.2681\n",
      "Train loss: 0.2665\n",
      "Train loss: 0.2708\n",
      "Train loss: 0.2733\n",
      "Train loss: 0.2713\n",
      "Train loss: 0.2672\n",
      "Train loss: 0.2635\n",
      "Train loss: 0.2578\n",
      "Train loss: 0.2569\n",
      "Train loss: 0.2582\n",
      "Train loss: 0.2584\n",
      "Train loss: 0.2594\n",
      "Train loss: 0.2576\n",
      "Train loss: 0.2599\n",
      "Train loss: 0.2773\n",
      "Train loss: 0.2722\n",
      "Train loss: 0.2700\n",
      "Train loss: 0.2600\n",
      "Train loss: 0.2608\n",
      "Train loss: 0.2692\n",
      "Train loss: 0.2616\n",
      "Train loss: 0.2609\n",
      "Train loss: 0.2592\n",
      "Train loss: 0.2578\n",
      "Train loss: 0.2567\n",
      "Train loss: 0.2565\n",
      "Train loss: 0.2623\n",
      "Train loss: 0.2622\n",
      "Train loss: 0.2588\n",
      "Train loss: 0.2568\n",
      "Train loss: 0.2578\n",
      "Train loss: 0.2577\n",
      "Train loss: 0.2603\n",
      "Train loss: 0.2574\n",
      "Train loss: 0.2606\n",
      "Train loss: 0.2603\n",
      "Train loss: 0.2560\n",
      "Train loss: 0.2578\n",
      "Train loss: 0.2578\n",
      "Test accuracy: 0.9150\n",
      "Train loss: 0.4186\n",
      "Train loss: 0.3474\n",
      "Train loss: 0.3100\n",
      "Train loss: 0.2996\n",
      "Train loss: 0.2763\n",
      "Train loss: 0.2705\n",
      "Train loss: 0.2663\n",
      "Train loss: 0.2648\n",
      "Train loss: 0.2619\n",
      "Train loss: 0.2610\n",
      "Train loss: 0.2630\n",
      "Train loss: 0.2608\n",
      "Train loss: 0.2640\n",
      "Train loss: 0.2606\n",
      "Train loss: 0.2618\n",
      "Train loss: 0.2649\n",
      "Train loss: 0.2576\n",
      "Train loss: 0.2591\n",
      "Train loss: 0.2619\n",
      "Train loss: 0.2608\n",
      "Train loss: 0.2653\n",
      "Train loss: 0.2653\n",
      "Train loss: 0.2582\n",
      "Train loss: 0.2626\n",
      "Train loss: 0.2662\n",
      "Train loss: 0.2571\n",
      "Train loss: 0.2590\n",
      "Train loss: 0.2607\n",
      "Train loss: 0.2583\n",
      "Train loss: 0.2698\n",
      "Train loss: 0.2595\n",
      "Train loss: 0.2573\n",
      "Train loss: 0.2635\n",
      "Train loss: 0.2582\n",
      "Train loss: 0.2558\n",
      "Train loss: 0.2637\n",
      "Train loss: 0.2630\n",
      "Train loss: 0.2661\n",
      "Train loss: 0.2593\n",
      "Train loss: 0.2581\n",
      "Train loss: 0.2599\n",
      "Train loss: 0.2571\n",
      "Train loss: 0.2594\n",
      "Train loss: 0.2560\n",
      "Train loss: 0.2580\n",
      "Train loss: 0.2567\n",
      "Train loss: 0.2561\n",
      "Train loss: 0.2582\n",
      "Train loss: 0.2627\n",
      "Train loss: 0.2617\n",
      "Test accuracy: 0.9100\n",
      "Train loss: 0.3993\n",
      "Train loss: 0.3408\n",
      "Train loss: 0.3355\n",
      "Train loss: 0.3095\n",
      "Train loss: 0.3056\n",
      "Train loss: 0.3126\n",
      "Train loss: 0.3141\n",
      "Train loss: 0.2774\n",
      "Train loss: 0.2799\n",
      "Train loss: 0.2837\n",
      "Train loss: 0.2754\n",
      "Train loss: 0.2680\n",
      "Train loss: 0.2803\n",
      "Train loss: 0.2693\n",
      "Train loss: 0.2653\n",
      "Train loss: 0.2729\n",
      "Train loss: 0.2725\n",
      "Train loss: 0.2677\n",
      "Train loss: 0.2748\n",
      "Train loss: 0.2681\n",
      "Train loss: 0.2766\n",
      "Train loss: 0.2778\n",
      "Train loss: 0.2676\n",
      "Train loss: 0.2590\n",
      "Train loss: 0.2650\n",
      "Train loss: 0.2620\n",
      "Train loss: 0.2633\n",
      "Train loss: 0.2668\n",
      "Train loss: 0.2592\n",
      "Train loss: 0.2613\n",
      "Train loss: 0.2616\n",
      "Train loss: 0.2573\n",
      "Train loss: 0.2655\n",
      "Train loss: 0.2650\n",
      "Train loss: 0.2580\n",
      "Train loss: 0.2595\n",
      "Train loss: 0.2611\n",
      "Train loss: 0.2591\n",
      "Train loss: 0.2605\n",
      "Train loss: 0.2573\n",
      "Train loss: 0.2640\n",
      "Train loss: 0.2598\n",
      "Train loss: 0.2586\n",
      "Train loss: 0.2618\n",
      "Train loss: 0.2618\n",
      "Train loss: 0.2604\n",
      "Train loss: 0.2574\n",
      "Train loss: 0.2577\n",
      "Train loss: 0.2642\n",
      "Train loss: 0.2588\n",
      "Test accuracy: 0.9100\n",
      "Train loss: 0.4061\n",
      "Train loss: 0.3511\n",
      "Train loss: 0.3516\n",
      "Train loss: 0.2904\n",
      "Train loss: 0.3055\n",
      "Train loss: 0.2782\n",
      "Train loss: 0.2832\n",
      "Train loss: 0.2891\n",
      "Train loss: 0.2991\n",
      "Train loss: 0.2903\n",
      "Train loss: 0.2806\n",
      "Train loss: 0.2739\n",
      "Train loss: 0.2707\n",
      "Train loss: 0.2632\n",
      "Train loss: 0.2636\n",
      "Train loss: 0.2631\n",
      "Train loss: 0.2611\n",
      "Train loss: 0.2652\n",
      "Train loss: 0.2595\n",
      "Train loss: 0.2615\n",
      "Train loss: 0.2585\n",
      "Train loss: 0.2581\n",
      "Train loss: 0.2646\n",
      "Train loss: 0.2711\n",
      "Train loss: 0.2678\n",
      "Train loss: 0.2626\n",
      "Train loss: 0.2636\n",
      "Train loss: 0.2598\n",
      "Train loss: 0.2627\n",
      "Train loss: 0.2606\n",
      "Train loss: 0.2610\n",
      "Train loss: 0.2582\n",
      "Train loss: 0.2579\n",
      "Train loss: 0.2577\n",
      "Train loss: 0.2590\n",
      "Train loss: 0.2601\n",
      "Train loss: 0.2574\n",
      "Train loss: 0.2566\n",
      "Train loss: 0.2573\n",
      "Train loss: 0.2595\n",
      "Train loss: 0.2627\n",
      "Train loss: 0.2617\n",
      "Train loss: 0.2629\n",
      "Train loss: 0.2671\n",
      "Train loss: 0.2622\n",
      "Train loss: 0.2668\n",
      "Train loss: 0.2618\n",
      "Train loss: 0.2578\n",
      "Train loss: 0.2569\n",
      "Train loss: 0.2642\n",
      "Test accuracy: 0.9200\n",
      "Train loss: 0.4118\n",
      "Train loss: 0.3272\n",
      "Train loss: 0.3145\n",
      "Train loss: 0.3087\n",
      "Train loss: 0.2781\n",
      "Train loss: 0.2864\n",
      "Train loss: 0.2889\n",
      "Train loss: 0.3166\n",
      "Train loss: 0.2682\n",
      "Train loss: 0.2713\n",
      "Train loss: 0.2807\n",
      "Train loss: 0.2853\n",
      "Train loss: 0.2669\n",
      "Train loss: 0.2732\n",
      "Train loss: 0.2669\n",
      "Train loss: 0.2739\n",
      "Train loss: 0.2642\n",
      "Train loss: 0.2699\n",
      "Train loss: 0.2673\n",
      "Train loss: 0.2623\n",
      "Train loss: 0.2861\n",
      "Train loss: 0.2618\n",
      "Train loss: 0.2586\n",
      "Train loss: 0.2710\n",
      "Train loss: 0.2708\n",
      "Train loss: 0.2577\n",
      "Train loss: 0.2578\n",
      "Train loss: 0.2610\n",
      "Train loss: 0.2622\n",
      "Train loss: 0.2565\n",
      "Train loss: 0.2579\n",
      "Train loss: 0.2570\n",
      "Train loss: 0.2601\n",
      "Train loss: 0.2570\n",
      "Train loss: 0.2583\n",
      "Train loss: 0.2577\n",
      "Train loss: 0.2576\n",
      "Train loss: 0.2572\n",
      "Train loss: 0.2622\n",
      "Train loss: 0.2644\n",
      "Train loss: 0.2718\n",
      "Train loss: 0.2696\n",
      "Train loss: 0.2706\n",
      "Train loss: 0.2584\n",
      "Train loss: 0.2578\n",
      "Train loss: 0.2608\n",
      "Train loss: 0.2571\n",
      "Train loss: 0.2580\n",
      "Train loss: 0.2568\n",
      "Train loss: 0.2576\n",
      "Test accuracy: 0.9300\n",
      "Train loss: 0.4054\n",
      "Train loss: 0.3287\n",
      "Train loss: 0.3072\n",
      "Train loss: 0.2829\n",
      "Train loss: 0.2778\n",
      "Train loss: 0.2697\n",
      "Train loss: 0.2659\n",
      "Train loss: 0.2755\n",
      "Train loss: 0.2639\n",
      "Train loss: 0.2607\n",
      "Train loss: 0.2595\n",
      "Train loss: 0.2621\n",
      "Train loss: 0.2634\n",
      "Train loss: 0.2615\n",
      "Train loss: 0.2632\n",
      "Train loss: 0.2645\n",
      "Train loss: 0.2580\n",
      "Train loss: 0.2567\n",
      "Train loss: 0.2565\n",
      "Train loss: 0.2649\n",
      "Train loss: 0.2617\n",
      "Train loss: 0.2661\n",
      "Train loss: 0.2748\n",
      "Train loss: 0.2687\n",
      "Train loss: 0.2657\n",
      "Train loss: 0.2607\n",
      "Train loss: 0.2564\n",
      "Train loss: 0.2637\n",
      "Train loss: 0.2587\n",
      "Train loss: 0.2609\n",
      "Train loss: 0.2592\n",
      "Train loss: 0.2604\n",
      "Train loss: 0.2621\n",
      "Train loss: 0.2602\n",
      "Train loss: 0.2615\n",
      "Train loss: 0.2584\n",
      "Train loss: 0.2581\n",
      "Train loss: 0.2590\n",
      "Train loss: 0.2583\n",
      "Train loss: 0.2609\n",
      "Train loss: 0.2598\n",
      "Train loss: 0.2600\n",
      "Train loss: 0.2560\n",
      "Train loss: 0.2572\n",
      "Train loss: 0.2686\n",
      "Train loss: 0.2704\n",
      "Train loss: 0.2567\n",
      "Train loss: 0.2564\n",
      "Train loss: 0.2569\n",
      "Train loss: 0.2624\n",
      "Test accuracy: 0.9000\n",
      "Train loss: 0.4074\n",
      "Train loss: 0.3791\n",
      "Train loss: 0.3582\n",
      "Train loss: 0.3277\n",
      "Train loss: 0.3066\n",
      "Train loss: 0.2923\n",
      "Train loss: 0.2914\n",
      "Train loss: 0.2846\n",
      "Train loss: 0.2676\n",
      "Train loss: 0.2713\n",
      "Train loss: 0.2698\n",
      "Train loss: 0.2667\n",
      "Train loss: 0.2754\n",
      "Train loss: 0.2593\n",
      "Train loss: 0.2606\n",
      "Train loss: 0.2669\n",
      "Train loss: 0.2620\n",
      "Train loss: 0.2593\n",
      "Train loss: 0.2584\n",
      "Train loss: 0.2572\n",
      "Train loss: 0.2574\n",
      "Train loss: 0.2609\n",
      "Train loss: 0.2634\n",
      "Train loss: 0.2589\n",
      "Train loss: 0.2598\n",
      "Train loss: 0.2629\n",
      "Train loss: 0.2690\n",
      "Train loss: 0.2597\n",
      "Train loss: 0.2674\n",
      "Train loss: 0.2600\n",
      "Train loss: 0.2607\n",
      "Train loss: 0.2564\n",
      "Train loss: 0.2574\n",
      "Train loss: 0.2690\n",
      "Train loss: 0.2607\n",
      "Train loss: 0.2619\n",
      "Train loss: 0.2600\n",
      "Train loss: 0.2631\n",
      "Train loss: 0.2581\n",
      "Train loss: 0.2562\n",
      "Train loss: 0.2603\n",
      "Train loss: 0.2587\n",
      "Train loss: 0.2573\n",
      "Train loss: 0.2605\n",
      "Train loss: 0.2612\n",
      "Train loss: 0.2591\n",
      "Train loss: 0.2571\n",
      "Train loss: 0.2614\n",
      "Train loss: 0.2562\n",
      "Train loss: 0.2560\n",
      "Test accuracy: 0.9050\n",
      "Train loss: 0.4155\n",
      "Train loss: 0.3320\n",
      "Train loss: 0.3255\n",
      "Train loss: 0.3051\n",
      "Train loss: 0.3001\n",
      "Train loss: 0.2840\n",
      "Train loss: 0.2741\n",
      "Train loss: 0.2741\n",
      "Train loss: 0.2660\n",
      "Train loss: 0.2660\n",
      "Train loss: 0.2619\n",
      "Train loss: 0.2618\n",
      "Train loss: 0.2611\n",
      "Train loss: 0.2611\n",
      "Train loss: 0.2579\n",
      "Train loss: 0.2602\n",
      "Train loss: 0.2631\n",
      "Train loss: 0.2568\n",
      "Train loss: 0.2585\n",
      "Train loss: 0.2599\n",
      "Train loss: 0.2594\n",
      "Train loss: 0.2614\n",
      "Train loss: 0.2561\n",
      "Train loss: 0.2609\n",
      "Train loss: 0.2581\n",
      "Train loss: 0.2577\n",
      "Train loss: 0.2575\n",
      "Train loss: 0.2625\n",
      "Train loss: 0.2691\n",
      "Train loss: 0.2598\n",
      "Train loss: 0.2596\n",
      "Train loss: 0.2603\n",
      "Train loss: 0.2646\n",
      "Train loss: 0.2648\n",
      "Train loss: 0.2565\n",
      "Train loss: 0.2562\n",
      "Train loss: 0.2586\n",
      "Train loss: 0.2600\n",
      "Train loss: 0.2597\n",
      "Train loss: 0.2595\n",
      "Train loss: 0.2584\n",
      "Train loss: 0.2655\n",
      "Train loss: 0.2710\n",
      "Train loss: 0.2648\n",
      "Train loss: 0.2588\n",
      "Train loss: 0.2577\n",
      "Train loss: 0.2626\n",
      "Train loss: 0.2590\n",
      "Train loss: 0.2609\n",
      "Train loss: 0.2566\n",
      "Test accuracy: 0.9200\n",
      "Train loss: 0.4210\n",
      "Train loss: 0.4049\n",
      "Train loss: 0.3391\n",
      "Train loss: 0.3018\n",
      "Train loss: 0.2903\n",
      "Train loss: 0.2945\n",
      "Train loss: 0.2990\n",
      "Train loss: 0.2797\n",
      "Train loss: 0.2818\n",
      "Train loss: 0.2678\n",
      "Train loss: 0.2655\n",
      "Train loss: 0.2601\n",
      "Train loss: 0.2624\n",
      "Train loss: 0.2625\n",
      "Train loss: 0.2581\n",
      "Train loss: 0.2578\n",
      "Train loss: 0.2591\n",
      "Train loss: 0.2631\n",
      "Train loss: 0.2603\n",
      "Train loss: 0.2672\n",
      "Train loss: 0.2656\n",
      "Train loss: 0.2668\n",
      "Train loss: 0.2835\n",
      "Train loss: 0.2660\n",
      "Train loss: 0.2737\n",
      "Train loss: 0.2788\n",
      "Train loss: 0.2597\n",
      "Train loss: 0.2600\n",
      "Train loss: 0.2648\n",
      "Train loss: 0.2616\n",
      "Train loss: 0.2598\n",
      "Train loss: 0.2667\n",
      "Train loss: 0.2607\n",
      "Train loss: 0.2591\n",
      "Train loss: 0.2597\n",
      "Train loss: 0.2609\n",
      "Train loss: 0.2706\n",
      "Train loss: 0.2883\n",
      "Train loss: 0.2656\n",
      "Train loss: 0.2565\n",
      "Train loss: 0.2600\n",
      "Train loss: 0.2611\n",
      "Train loss: 0.2650\n",
      "Train loss: 0.2661\n",
      "Train loss: 0.2672\n",
      "Train loss: 0.2651\n",
      "Train loss: 0.2582\n",
      "Train loss: 0.2561\n",
      "Train loss: 0.2589\n",
      "Train loss: 0.2572\n",
      "Test accuracy: 0.9300\n",
      "Train loss: 0.3858\n",
      "Train loss: 0.3128\n",
      "Train loss: 0.2942\n",
      "Train loss: 0.2912\n",
      "Train loss: 0.2845\n",
      "Train loss: 0.2800\n",
      "Train loss: 0.2741\n",
      "Train loss: 0.2683\n",
      "Train loss: 0.2761\n",
      "Train loss: 0.2712\n",
      "Train loss: 0.2697\n",
      "Train loss: 0.2754\n",
      "Train loss: 0.2729\n",
      "Train loss: 0.2698\n",
      "Train loss: 0.2632\n",
      "Train loss: 0.2726\n",
      "Train loss: 0.2779\n",
      "Train loss: 0.2614\n",
      "Train loss: 0.2621\n",
      "Train loss: 0.2597\n",
      "Train loss: 0.2646\n",
      "Train loss: 0.2580\n",
      "Train loss: 0.2673\n",
      "Train loss: 0.2587\n",
      "Train loss: 0.2575\n",
      "Train loss: 0.2612\n",
      "Train loss: 0.2572\n",
      "Train loss: 0.2630\n",
      "Train loss: 0.2589\n",
      "Train loss: 0.2567\n",
      "Train loss: 0.2599\n",
      "Train loss: 0.2563\n",
      "Train loss: 0.2630\n",
      "Train loss: 0.2601\n",
      "Train loss: 0.2656\n",
      "Train loss: 0.2628\n",
      "Train loss: 0.2614\n",
      "Train loss: 0.2590\n",
      "Train loss: 0.2748\n",
      "Train loss: 0.2605\n",
      "Train loss: 0.2628\n",
      "Train loss: 0.2711\n",
      "Train loss: 0.2769\n",
      "Train loss: 0.2656\n",
      "Train loss: 0.2641\n",
      "Train loss: 0.2615\n",
      "Train loss: 0.2692\n",
      "Train loss: 0.2610\n",
      "Train loss: 0.2573\n",
      "Train loss: 0.2673\n",
      "Test accuracy: 0.9400\n",
      "Average test accuracy over 10 runs: 0.9180 ± 0.0119\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "for _ in range(10):\n",
    "    w, losses = pac_private_logistic_regression(X_train, y_train, mu=1e-3, mi_budget=1/1024, T=50, privacy_aware=True)\n",
    "    test_accuracy = evaluate_model(X_test, y_test, w)\n",
    "    accs.append(test_accuracy)\n",
    "\n",
    "    print(f'Test accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "print(f'Average test accuracy over 10 runs: {np.mean(accs):.4f} ± {np.std(accs):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pac_lr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
